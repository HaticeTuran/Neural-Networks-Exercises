{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdbKICTzzE1EkukoCsNK5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaticeTuran/Neural-Networks-Exercises/blob/main/NN_UE4_Hatice_Turan_190503011.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iLpROGXcDk3k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnFHmKvsC15s"
      },
      "source": [
        "## 10.\n",
        "_Exercise: In this exercise you will download a dataset, split it, create a `tf.data.Dataset` to load it and preprocess it efficiently, then build and train a binary classification model containing an `Embedding` layer._\n",
        "\n",
        "### a.\n",
        "_Exercise: Download the [Large Movie Review Dataset](https://homl.info/imdb), which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/). The data is organized in two directories, `train` and `test`, each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cHDhxZ99C15s",
        "outputId": "b50b17b6-696c-427d-d27e-36c22237ab72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 6s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.keras/datasets/aclImdb')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "import tensorflow \n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "FILENAME = \"aclImdb_v1.tar.gz\"\n",
        "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
        "path = Path(filepath).parent / \"aclImdb\"\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Oxd6QHdAC15t",
        "outputId": "01bc5305-f11e-4899-e3ea-479d1e554c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb/\n",
            "    README\n",
            "    imdb.vocab\n",
            "    imdbEr.txt\n",
            "    train/\n",
            "        labeledBow.feat\n",
            "        unsupBow.feat\n",
            "        urls_neg.txt\n",
            "        ...\n",
            "        neg/\n",
            "            0_3.txt\n",
            "            10000_4.txt\n",
            "            10001_4.txt\n",
            "            ...\n",
            "        unsup/\n",
            "            0_0.txt\n",
            "            10000_0.txt\n",
            "            10001_0.txt\n",
            "            ...\n",
            "        pos/\n",
            "            0_9.txt\n",
            "            10000_8.txt\n",
            "            10001_10.txt\n",
            "            ...\n",
            "    test/\n",
            "        labeledBow.feat\n",
            "        urls_neg.txt\n",
            "        urls_pos.txt\n",
            "        neg/\n",
            "            0_2.txt\n",
            "            10000_4.txt\n",
            "            10001_1.txt\n",
            "            ...\n",
            "        pos/\n",
            "            0_10.txt\n",
            "            10000_7.txt\n",
            "            10001_9.txt\n",
            "            ...\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "for name, subdirs, files in os.walk(path):\n",
        "    indent = len(Path(name).parts) - len(path.parts)\n",
        "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
        "    for index, filename in enumerate(sorted(files)):\n",
        "        if index == 3:\n",
        "            print(\"    \" * (indent + 1) + \"...\")\n",
        "            break\n",
        "        print(\"    \" * (indent + 1) + filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RQMyZNnEC15u",
        "outputId": "7084a558-3dea-4d41-9481-3250482c23cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500, 12500, 12500)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def review_paths(dirpath):\n",
        "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
        "\n",
        "train_pos = review_paths(path / \"train\" / \"pos\")\n",
        "train_neg = review_paths(path / \"train\" / \"neg\")\n",
        "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
        "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
        "\n",
        "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw6Mkr0qC15u"
      },
      "source": [
        "### b.\n",
        "_Exercise: Split the test set into a validation set (15,000) and a test set (10,000)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yFDECn4lC15v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.shuffle(test_valid_pos)\n",
        "\n",
        "test_pos = test_valid_pos[:5000]\n",
        "test_neg = test_valid_neg[:5000]\n",
        "valid_pos = test_valid_pos[5000:]\n",
        "valid_neg = test_valid_neg[5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZQftNr2C15v"
      },
      "source": [
        "### c.\n",
        "_Exercise: Use tf.data to create an efficient dataset for each set._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Far5zPHYC15v"
      },
      "source": [
        "Since the dataset fits in memory, we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "716K_s78C15w"
      },
      "outputs": [],
      "source": [
        "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
        "        for filepath in filepaths:\n",
        "            with open(filepath) as review_file:\n",
        "                reviews.append(review_file.read())\n",
        "            labels.append(label)\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        (tf.constant(reviews), tf.constant(labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TcLTm9i_C15w",
        "outputId": "40e0ba12-1b6f-4ade-b95f-810c8dc39443",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"I expected this to be a lot better. I love Tim Burton's work, so I was really excited to see these online short films. Well, they weren't at all what I had expected.<br /><br />I don't really know what exactly it is I don't like. I guess they're just sort of dull. The sound bothers me, and most of the characters, although I loved Roy the Toxic Boy, and Stainboy.<br /><br />The Match Girl episode probably bugged me the most, although it was pretty funny.<br /><br />I also don't like the way some of the characters die. Like how Match Girl basically set the gas station on fire, or how the Girl Who Stares died, in general. Roy's death was amusing, surprisingly. Death by a car freshener. Very original ;-) That made me laugh so hard...<br /><br />There are some things that aren't appropriate for kids. Just some language and gore. That's about all I have to say! 3/10\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n",
            "tf.Tensor(b\"There can be no questions of spoilers for this movie, the director beat us all too and spoiled this movie in oh so many ways.<br /><br />A blatant rip-off of stuff like Critters and Gremlins, this movie fails on so many levels to recapture the humour and horror of those better made films. It ends up a sleazy waste of time, where bad actors deliver bad dialogue in front of an idiot director, who occasionally tosses stuffed toys at them. They wrestle with said toys in much the same manner as old Tarzan films used to use rubber crocodiles, shaking them whilst screaming and trying their best to make it look slightly threatening. It's painful to watch, and not helped by the mental 80's fashions worn by the cast.<br /><br />Basically, some crazy little aliens who have been trapped by an aging security guard in a film lot finally get free after umpteen years confinement, and begin to telepathically screw around with peoples minds. The guards new recruit, the idiot who let them out despite repeated warnings, gets his gang of 80's friends together and they go off and have minor adventures together while trying to recapture the Grem... Hobgoblins.<br /><br />All life is here, with the gang consisting of a knucklehead jock, his 80's slut girlfriend, the 'hero's frigid and prissy girlfriend, and the young hero, lacking in confidence and wishing his girlfriend would put out anyway.<br /><br />First off comes the infamous rake fighting scene, where the ex-military jock shows how he was trained in the army to be a bully, poking the nerdy hero with the wrong end of a rake for what seems like hours. Then there's some running around, terminating in a real pie-fight style ending in a scuzzy nightclub with comedy hand-grenades blowing up everything except the people standing right next to them. Then the film sorta ends, and alls well that ends well.<br /><br />It's not. This is like watching a train wreck, you cant take your eyes off it, it's so bad. Perfect fare for Mystery Science Theater, but god-awful should you try to watch it alone and uncut. The Fashion Police still have a number of outstanding warrants for the cast, and I dare anyone not to laugh in outright derision at the rake fight. This scores 2 out of 10 at most, on a good day.\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n",
            "tf.Tensor(b\"I think I will make a movie next weekend. Oh wait, I'm working..oh I'm sure I can fit it in. It looks like whoever made this film fit it in. I hope the makers of this crap have day jobs because this film sucked!!! It looks like someones home movie and I don't think more than $100 was spent making it!!! Total crap!!! Who let's this stuff be released?!?!?!\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
        "    print(X)\n",
        "    print(y)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MNM1OiusC15x",
        "outputId": "ba028af0-6065-4d0c-b083-e96a070effac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ooRXtoPC15x"
      },
      "source": [
        "It takes about 59 seconds to load the dataset and go through it 10 times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFopmalRC15x"
      },
      "source": [
        "But let's pretend the dataset does not fit in memory, just to make things more interesting. Luckily, each review fits on just one line (they use `<br />` to indicate line breaks), so we can read the reviews using a `TextLineDataset`. If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords). For very large datasets, it would make sense to use a tool like Apache Beam for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6ia2xM4VC15y"
      },
      "outputs": [],
      "source": [
        "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
        "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
        "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
        "                                          num_parallel_reads=n_read_threads)\n",
        "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
        "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hbi_pP4KC15y",
        "outputId": "c1090e49-1edb-4357-ff7c-88a9c9d36b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2min 22s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7b6TaUUC15y"
      },
      "source": [
        "Now it takes about 2min 22s to go through the dataset 10 times. That's much slower, essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch. If you add `.cache()` just before `.repeat(10)`, you will see that this implementation will be about as fast as the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZnRtaY1tC15z",
        "outputId": "06609dfc-29a7-419e-885b-4fabe4764d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1min 22s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY9JBfRgC15z"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
        "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
        "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
      ]
    }
  ]
}